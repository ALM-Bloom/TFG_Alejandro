{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c08a836",
   "metadata": {},
   "source": [
    "### Carga de librerías necesarias\n",
    "\n",
    "Se importan los principales paquetes y módulos necesarios para el ajuste fino del modelo de lenguaje:\n",
    "\n",
    "- `transformers`: Para la carga del modelo base, tokenizador y configuración del entrenamiento.\n",
    "- `peft`: Librería para técnicas de ajuste fino eficiente como LoRA.\n",
    "- `torch` y `os`: Manejo de operaciones básicas con PyTorch y sistema de archivos.\n",
    "- `datasets`: Para manipulación y carga del conjunto de datos.\n",
    "- `trl`: Se importa exclusivamente el módulo `SFTTrainer` (Supervised Fine-Tuning Trainer).\n",
    "- `pandas` y `pyarrow`: Utilizadas para tratamiento y compatibilidad de datos tabulares.\n",
    "- `re`: Expresiones regulares utilizadas para procesamiento textual.\n",
    "- `accelerate`: Herramientas para facilitar la distribución del modelo entre dispositivos.\n",
    "- `json`: Para leer y transformar estructuras de datos en formato JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os, torch\n",
    "from datasets import load_datase, Dataset\n",
    "from trl import SFTTrainer \n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from datasets import Dataset\n",
    "import re\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f95489",
   "metadata": {},
   "source": [
    "### Preparación y carga del conjunto de datos\n",
    "\n",
    "Se define la ruta del modelo base (`Mistral-7B-Instruct-v0.2`) y el nombre del nuevo modelo especializado (`Oswestry-Instruct`).\n",
    "\n",
    "A continuación, se implementa una función `load_dataset()` que:\n",
    "- Carga el archivo JSON con los datos generados para el ajuste fino.\n",
    "- Formatea cada entrada conforme al formato esperado por el modelo que ocupamos (formato de conversación con tokens especiales `<s>[INST] ... [/INST] ... </s>`).\n",
    "- Devuelve un `Dataset` de HuggingFace estructurado para el entrenamiento.\n",
    "\n",
    "El archivo cargado contiene las entrevistas médico-paciente generadas con ChatGPT junto con la correspondiente evaluación esperada también generada sintéticamente. \n",
    "\n",
    "Posteriormente, el conjunto de datos completo se divide automáticamente en dos subconjuntos:\n",
    "- **80 % para entrenamiento** (`train_dataset`)\n",
    "- **20 % para validación** (`eval_dataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "new_model = \"Oswestry-Instruct\"\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    formatted_data = []\n",
    "    for item in data:\n",
    "        text = f\"<s>[INST] {item['instruction']} {item['input']} [/INST] {item['output']} </s>\"\n",
    "        formatted_data.append({\"text\": text})\n",
    "\n",
    "    return Dataset.from_list(formatted_data)\n",
    "\n",
    "dataset_path = \"Aquí va la ruta al archivo JSON con los datos de ajuste fino\"  # Reemplazar con la ruta correcta\n",
    "\n",
    "full_dataset = load_dataset(dataset_path)\n",
    "dataset = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b463c83",
   "metadata": {},
   "source": [
    "### Autenticación con Hugging Face\n",
    "\n",
    "Para poder cargar y subir modelos desde y hacia la plataforma Hugging Face, es necesario autenticarse mediante un token personal de acceso.\n",
    "\n",
    "Dentro de Kaggle, se accede de forma segura al token usando la clase `UserSecretsClient`, que permite recuperar secretos almacenados de forma privada.\n",
    "\n",
    "- Se obtiene el token de acceso personal mediante `user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")`.\n",
    "- Se realiza el login automático usando el comando `huggingface-cli login`.\n",
    "\n",
    "Esta autenticación es un paso previo obligatorio para utilizar repositorios privados o realizar `push_to_hub()` al finalizar el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "!huggingface-cli login --token $secret_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20dfc5d",
   "metadata": {},
   "source": [
    "### Carga del modelo base y configuración de cuantización\n",
    "\n",
    "En esta sección se carga el modelo base `Mistral-7B-Instruct-v0.2`, una variante preentrenada orientada a tareas instruccionales. Para hacer viable el ajuste fino en entornos con recursos limitados como Kaggle, se aplica cuantización extrema a 4 bits usando la librería `BitsAndBytes`.\n",
    "\n",
    "- **Cuantización a 4 bits (`load_in_4bit=True`)**: reduce drásticamente el uso de memoria, permitiendo entrenar modelos grandes.\n",
    "- **Tipo de cuantización `nf4`**: formato que mantiene buena precisión incluso en baja resolución.\n",
    "- **Cómputo en `bfloat16`**: mejora el aprovechamiento de la GPU sin degradar significativamente el rendimiento.\n",
    "- **`device_map=\"auto\"`**: distribuye automáticamente las capas del modelo entre los dispositivos disponibles.\n",
    "\n",
    "Posteriormente, se realiza la configuración del modelo para entrenamiento eficiente:\n",
    "- Se desactiva la caché con `use_cache = False` para permitir el uso de checkpointing de gradientes.\n",
    "- Se activa `gradient_checkpointing`, una técnica que reduce el uso de memoria durante el entrenamiento.\n",
    "- Se carga el tokenizador correspondiente al modelo, ajustando el padding y añadiendo el token de fin de secuencia (`eos_token`) como marcador de relleno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982003da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración de la cuantización\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= True,\n",
    ")\n",
    "\n",
    "# Cargamos el modelo base con la configuración de cuantización\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        load_in_4bit=True,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Carga del tokenizador\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model, \n",
    "    trust_remote_code=True,\n",
    "    use_fast=False \n",
    ")\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c262cf",
   "metadata": {},
   "source": [
    "### Preparación del modelo para ajuste fino con LoRA\n",
    "\n",
    "1. `prepare_model_for_kbit_training(model)`: prepara el modelo previamente cuantizado para ser entrenado con técnicas PEFT, asegurando compatibilidad con estructuras de baja precisión.\n",
    "\n",
    "2. **Configuración LoRA (`LoraConfig`)**:\n",
    "   - `r=64`: rango de la descomposición de matrices, controlando la capacidad de adaptación del modelo.\n",
    "   - `lora_alpha=16`: factor de escalado de las actualizaciones LoRA.\n",
    "   - `lora_dropout=0.1`: probabilidad de aplicar dropout a las capas LoRA.\n",
    "   - `bias=\"none\"`: no se actualizan los sesgos durante el entrenamiento.\n",
    "   - `task_type=\"CAUSAL_LM\"`: especifica que se está trabajando con un modelo de lenguaje causal.\n",
    "   - `target_modules`: define las proyecciones de atención sobre las cuales se aplicará la adaptación (claves, valores, consultas, salida y compuerta).\n",
    "\n",
    "3. `get_peft_model(...)`: aplica la configuración LoRA al modelo cargado, encapsulándolo en una nueva estructura entrenable con un número reducido de parámetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9829749",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bdc81",
   "metadata": {},
   "source": [
    "### Definición de los hiperparámetros de entrenamiento\n",
    "\n",
    "Se establece la configuración de entrenamiento mediante la clase `TrainingArguments`, adaptada a las limitaciones computacionales del entorno. \n",
    "\n",
    "- **Epochs**: Solo se realiza una época de entrenamiento (`num_train_epochs=1`), ya que se parte de un modelo previamente preentrenado y se busca una adaptación eficiente con un dataset de tamaño reducido.\n",
    "- **Batch Size**: Se utiliza un tamaño de lote de 4 (`per_device_train_batch_size=4`) y se activa la acumulación de gradientes (`gradient_accumulation_steps=1`) para mejorar el uso de memoria.\n",
    "- **Optimizador**: Se selecciona `paged_adamw_32bit`, diseñado para eficiencia en modelos cuantizados.\n",
    "- **Precisión Mixta**: Se activa tanto `fp16=True` como `bf16=True` para maximizar la compatibilidad y el rendimiento en entornos con soporte mixto.\n",
    "- **Evaluación**: Se realiza una evaluación periódica cada 50 pasos (`eval_steps=50`) y se registran métricas clave en la plataforma Weights & Biases (`report_to=\"wandb\"`).\n",
    "- **Programador de tasa de aprendizaje**: Se opta por un scheduler constante con una fase inicial de calentamiento (`warmup_ratio=0.03`) para estabilizar el entrenamiento.\n",
    "- **Ajustes Triviales**: Se incluye el recorte de gradiente (`max_grad_norm=0.3`), agrupación por longitud de secuencia (`group_by_length=True`) y un directorio específico para logs (`logging_dir=\"./logs\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21759286",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=50,\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    evaluation_strategy=\"steps\",   \n",
    "    eval_steps=50,                \n",
    "    logging_dir=\"./logs\",          \n",
    "    report_to=\"wandb\",             \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e7f5e",
   "metadata": {},
   "source": [
    "### Configuración del entrenador supervisado (SFTTrainer)\n",
    "\n",
    "Se inicializa el entrenador supervisado mediante la clase `SFTTrainer` de la librería `trl`, diseñada específicamente para realizar ajustes finos en modelos de lenguaje utilizando el enfoque PEFT (Parameter-Efficient Fine-Tuning).\n",
    "\n",
    "Parámetros principales:\n",
    "- **`model`**: Modelo base ya preparado con la configuración LoRA aplicada.\n",
    "- **`train_dataset` y `eval_dataset`**: Conjuntos de datos de entrenamiento y validación, previamente procesados y divididos.\n",
    "- **`peft_config`**: Configuración LoRA que define el rango, el dropout y los módulos del modelo a ajustar.\n",
    "- **`tokenizer`**: Tokenizador correspondiente al modelo utilizado.\n",
    "- **`args`**: Conjunto de hiperparámetros definidos en `TrainingArguments`.\n",
    "- **`dataset_text_field`**: Se especifica que el campo \"text\" contiene las entradas ya formateadas del dataset.\n",
    "- **`max_seq_length`**: No se impone una longitud máxima explícita, permitiendo usar el valor por defecto del modelo.\n",
    "- **`packing`**: Se desactiva la agrupación de múltiples muestras en una sola secuencia (desactivado por defecto para tareas de generación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72604c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,  \n",
    "    eval_dataset=eval_dataset, \n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7ec31",
   "metadata": {},
   "source": [
    "### Entrenamiento, guardado y despliegue del modelo\n",
    "\n",
    "- **Autenticación en Weights & Biases (W&B)**: Se realiza el login en la plataforma de monitorización con una clave API segura obtenida desde `kaggle_secrets`. Esto permite registrar métricas clave durante el entrenamiento.\n",
    "\n",
    "- **Entrenamiento del modelo**: Se lanza el ajuste fino mediante `trainer.train()`, utilizando los parámetros, datasets y modelo previamente definidos.\n",
    "\n",
    "- **Guardado del modelo ajustado**: Una vez completado el entrenamiento, se guarda el modelo con `save_pretrained(new_model)`, conservando únicamente los pesos LoRA especializados.\n",
    "\n",
    "- **Configuración para inferencia**: Se vuelve a activar la caché de resultados (`use_cache = True`) y se establece el modelo en modo evaluación (`model.eval()`).\n",
    "\n",
    "- **Publicación en Hugging Face Hub**:\n",
    "  - El modelo especializado se sube a Hugging Face Hub con `push_to_hub()`.\n",
    "  - También se sube el tokenizer correspondiente para garantizar compatibilidad en el uso futuro del modelo.\n",
    "\n",
    "Con esta última celda se concluye el proceso de ajuste fino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "secret_value_1 = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "wandb.login(key=secret_value_1)\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(new_model)\n",
    "model.config.use_cache = True\n",
    "model.eval()\n",
    "trainer.model.push_to_hub(new_model)\n",
    "tokenizer.push_to_hub(\"nombre_modelo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
